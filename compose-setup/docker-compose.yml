services:
  mcp:
    build:
      context: ./mcp-server
      dockerfile: Dockerfile
    container_name: genai-mcp-server
    ports:
      - "5005:5005"
      - "6006:6006"
    environment:
      - HOST=0.0.0.0
      - REST_API_PORT=5005
      - MCP_PORT=6006
      - LLM_API_URL=http://ollama:11434
      - LLM_MODEL=llama3.2:3b          # match the model defined in ollama service
    restart: unless-stopped

  frontend:
    build:
      context: ./react-app
      dockerfile: Dockerfile
    container_name: genai-react-app
    ports:
      - "5050:5050"
    environment:
      - HOST=0.0.0.0
      - PORT=5050
      - API_BASE_URL=http://mcp:5005
    restart: unless-stopped

  ollama:
    build:
      context: ./ollama
    container_name: genai-ollama
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
      - OLLAMA_MODEL=llama3.2:3b   # pick your small model
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama       # IMPORTANT: persists the pulled model
    restart: unless-stopped

volumes:
  ollama-data:
