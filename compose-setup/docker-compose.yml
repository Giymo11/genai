services:
  mcp:
    build:
      context: ./mcp-server
      dockerfile: Dockerfile
    container_name: genai-mcp-server
    ports:
      - "5005:5005"
      - "6006:6006"
    environment:
      - HOST=0.0.0.0
      - REST_API_PORT=5005
      - MCP_PORT=6006
      - CHROMA_HOST=chromadb
      - CHROMA_PORT=8000
      - LLM_API_URL=http://ollama:11434
      - LLM_MODEL=llama3.2:3b          # match the model defined in ollama service
    restart: unless-stopped
    volumes:
      - ../data:/app/data

  frontend:
    build:
      context: ./react-app
      dockerfile: Dockerfile
    container_name: genai-react-app
    ports:
      - "5050:5050"
    environment:
      - HOST=0.0.0.0
      - PORT=5050
      - API_BASE_URL=http://mcp:5005
    restart: unless-stopped

  ollama:
    build:
      context: ./ollama
    container_name: genai-ollama
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
      - OLLAMA_MODEL=llama3.2:3b   # pick your small model
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama       # IMPORTANT: persists the pulled model
    restart: unless-stopped

  chromadb:
    image: chromadb/chroma:latest
    container_name: genai-chromadb
    ports:
      - "8001:8000"
    volumes:
      - chroma_data:/chroma

volumes:
  chroma_data:
  ollama-data:
